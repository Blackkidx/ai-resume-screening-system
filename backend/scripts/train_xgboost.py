# -*- coding: utf-8 -*-
"""
ğŸ¤– XGBoost Training Pipeline â€” AI Resume Screening System

à¸£à¸±à¸™: python backend/scripts/train_xgboost.py

à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ HR decisions à¸ˆà¸²à¸ MongoDB â†’ Train XGBoost â†’ à¸šà¸±à¸™à¸—à¸¶à¸ model
"""

import asyncio
import json
import os
import sys
import traceback
from datetime import datetime
from pathlib import Path

# Fix Windows encoding (cp1252 â†’ utf-8) for emoji support
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding="utf-8", errors="replace")
    sys.stderr.reconfigure(encoding="utf-8", errors="replace")

import joblib
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from dotenv import load_dotenv
from sklearn.metrics import (
    accuracy_score,
    auc,
    classification_report,
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score,
    roc_auc_score,
)
from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Config
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FEATURE_NAMES = ["skills", "major", "experience", "projects", "certification", "gpa"]
MIN_SAMPLES = 20
TEST_SIZE = 0.2
RANDOM_STATE = 42
CV_FOLDS = 5

# Paths
ROOT_DIR = Path(__file__).resolve().parent.parent
MODELS_DIR = ROOT_DIR / "models"
ENV_PATH = ROOT_DIR / ".env"

MODEL_PATH = MODELS_DIR / "xgboost_model.pkl"
SCALER_PATH = MODELS_DIR / "xgboost_scaler.pkl"
METADATA_PATH = MODELS_DIR / "xgboost_metadata.json"
CHART_PATH = MODELS_DIR / "feature_importance.png"


def print_header():
    """à¹à¸ªà¸”à¸‡ header à¸•à¸­à¸™à¹€à¸£à¸´à¹ˆà¸¡ script"""
    print()
    print("ğŸš€ XGBoost Training Pipeline")
    print("â”" * 50)
    print(f"ğŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“ Models dir: {MODELS_DIR}")
    print("â”" * 50)
    print()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 1: Load data from MongoDB
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def load_data_from_mongodb() -> list[dict]:
    """à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ applications à¸—à¸µà¹ˆ HR à¸•à¸±à¸”à¸ªà¸´à¸™à¹à¸¥à¹‰à¸§à¸ˆà¸²à¸ MongoDB"""
    import motor.motor_asyncio

    load_dotenv(ENV_PATH)
    mongo_url = os.getenv("MONGODB_URL", "mongodb://localhost:27017")
    db_name = os.getenv("DATABASE_NAME", "ai_resume_screening")

    print("ğŸ“Š Loading data from MongoDB...")
    print(f"   Database: {db_name}")

    client = motor.motor_asyncio.AsyncIOMotorClient(mongo_url)
    db = client[db_name]

    # Query applications à¸—à¸µà¹ˆà¸¡à¸µ hr_decision
    cursor = db.applications.find(
        {"hr_decision": {"$in": ["accepted", "rejected"]}},
        {
            "hr_decision": 1,
            "ai_breakdown_at_decision": 1,
            "ai_breakdown": 1,
            "matching_breakdown": 1,
            "ai_score_at_decision": 1,
            "ai_overall_score": 1,
        },
    )
    applications = await cursor.to_list(length=None)
    client.close()

    return applications


def extract_features(applications: list[dict]) -> tuple[pd.DataFrame, pd.Series]:
    """à¹à¸›à¸¥à¸‡ application documents â†’ features DataFrame + labels"""
    rows = []
    skipped = 0

    for app in applications:
        # à¸”à¸¶à¸‡ breakdown: ai_breakdown_at_decision â†’ matching_breakdown â†’ ai_breakdown
        breakdown = (
            app.get("ai_breakdown_at_decision")
            or app.get("matching_breakdown")
            or app.get("ai_breakdown")
        )
        if not breakdown:
            skipped += 1
            continue

        # à¸”à¸¶à¸‡ 6 features
        features = {}
        valid = True
        for name in FEATURE_NAMES:
            value = breakdown.get(name)
            if value is None:
                valid = False
                break
            try:
                features[name] = float(value)
            except (ValueError, TypeError):
                valid = False
                break

        if not valid:
            skipped += 1
            continue

        # Label
        label = 1 if app["hr_decision"] == "accepted" else 0
        features["label"] = label
        rows.append(features)

    if skipped > 0:
        print(f"   âš ï¸ Skipped {skipped} applications (missing breakdown data)")

    if not rows:
        return pd.DataFrame(), pd.Series(dtype=int)

    df = pd.DataFrame(rows)
    X = df[FEATURE_NAMES]
    y = df["label"]
    return X, y


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 2-3: Validate data
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def validate_data(X: pd.DataFrame, y: pd.Series) -> bool:
    """à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸ˆà¸³à¸™à¸§à¸™à¹à¸¥à¸°à¸ªà¸¡à¸”à¸¸à¸¥à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥"""
    total = len(y)
    accepted = int(y.sum())
    rejected = total - accepted

    print(f"âœ… Found {total} applications ({accepted} accepted, {rejected} rejected)")

    if total < MIN_SAMPLES:
        print(f"\nâš ï¸  Not enough data! Need at least {MIN_SAMPLES}, got {total}")
        print("   â†’ à¹ƒà¸«à¹‰ HR à¸•à¸±à¸”à¸ªà¸´à¸™à¹ƒà¸ˆà¹€à¸à¸´à¹ˆà¸¡à¹à¸¥à¹‰à¸§à¸„à¹ˆà¸­à¸¢ train à¹ƒà¸«à¸¡à¹ˆ")
        return False

    # Check class balance
    ratio = min(accepted, rejected) / max(accepted, rejected) if max(accepted, rejected) > 0 else 0
    if ratio < 0.2:
        print(f"   âš ï¸ Class imbalance warning: ratio = {ratio:.2f}")
        print(f"      Accepted: {accepted}, Rejected: {rejected}")
        print("      â†’ à¸ˆà¸°à¹ƒà¸Šà¹‰ scale_pos_weight à¹€à¸à¸·à¹ˆà¸­à¸Šà¸”à¹€à¸Šà¸¢")

    return True


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 4-10: Train & Evaluate
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def train_and_evaluate(
    X: pd.DataFrame, y: pd.Series
) -> dict:
    """Train XGBoost + evaluate + return metrics"""

    accepted_count = int(y.sum())
    rejected_count = len(y) - accepted_count

    # Step 5: StandardScaler
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Step 6: Train/Test split
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE
    )

    # Step 7: Train XGBClassifier
    spw = rejected_count / accepted_count if accepted_count > 0 else 1.0

    print("\nğŸ“ˆ Training XGBoost...")
    print(f"   Train size: {len(X_train)}, Test size: {len(X_test)}")
    print(f"   scale_pos_weight: {spw:.2f}")

    model = XGBClassifier(
        objective="binary:logistic",
        n_estimators=100,
        max_depth=4,
        learning_rate=0.1,
        random_state=RANDOM_STATE,
        scale_pos_weight=spw,
        eval_metric="logloss",
        use_label_encoder=False,
    )
    model.fit(X_train, y_train, verbose=False)
    print("âœ… Training complete!")

    # Step 8: Evaluate
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, zero_division=0)
    rec = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)

    try:
        auc_roc = roc_auc_score(y_test, y_prob)
    except ValueError:
        auc_roc = 0.0

    print("\nğŸ“Š Evaluation Results:")
    print(f"   Accuracy:  {acc:.1%}")
    print(f"   Precision: {prec:.1%}")
    print(f"   Recall:    {rec:.1%}")
    print(f"   F1-Score:  {f1:.1%}")
    print(f"   AUC-ROC:   {auc_roc:.3f}")

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    print(f"\nğŸ“‹ Confusion Matrix:")
    print(f"   {cm}")

    # Classification Report
    print(f"\nğŸ“‹ Classification Report:")
    print(classification_report(y_test, y_pred, target_names=["rejected", "accepted"]))

    # Step 9: Cross-Validation
    print(f"ğŸ“Š Cross-Validation ({CV_FOLDS}-fold):")
    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)
    cv_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring="accuracy")
    cv_mean = cv_scores.mean()
    cv_std = cv_scores.std()
    print(f"   Mean Accuracy: {cv_mean:.1%} (Â± {cv_std:.1%})")

    # Step 10: Feature Importance
    importance = model.feature_importances_
    importance_dict = {
        name: round(float(imp), 4)
        for name, imp in sorted(
            zip(FEATURE_NAMES, importance), key=lambda x: x[1], reverse=True
        )
    }

    print("\nğŸ† Feature Importance:")
    for name, imp in importance_dict.items():
        bar = "â–ˆ" * int(imp * 30)
        print(f"   {name:14s}: {bar} {imp:.4f}")

    return {
        "model": model,
        "scaler": scaler,
        "metrics": {
            "accuracy": round(float(acc), 4),
            "precision": round(float(prec), 4),
            "recall": round(float(rec), 4),
            "f1_score": round(float(f1), 4),
            "auc_roc": round(float(auc_roc), 4),
            "cv_mean_accuracy": round(float(cv_mean), 4),
            "cv_std": round(float(cv_std), 4),
        },
        "feature_importance": importance_dict,
        "total_samples": len(y),
        "accepted_count": accepted_count,
        "rejected_count": rejected_count,
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 11-13: Save model & artifacts
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_model(result: dict):
    """à¸šà¸±à¸™à¸—à¸¶à¸ model, scaler, metadata, chart"""
    MODELS_DIR.mkdir(parents=True, exist_ok=True)

    # Step 11: Save model + scaler
    joblib.dump(result["model"], MODEL_PATH)
    joblib.dump(result["scaler"], SCALER_PATH)
    print(f"\nğŸ’¾ Model saved: {MODEL_PATH.name}")
    print(f"ğŸ’¾ Scaler saved: {SCALER_PATH.name}")

    # Step 12: Save metadata
    metadata = {
        "trained_at": datetime.now().isoformat(),
        "total_samples": result["total_samples"],
        "accepted_count": result["accepted_count"],
        "rejected_count": result["rejected_count"],
        **result["metrics"],
        "feature_names": FEATURE_NAMES,
        "feature_importance": result["feature_importance"],
    }
    METADATA_PATH.write_text(json.dumps(metadata, indent=2, ensure_ascii=False), encoding="utf-8")
    print(f"ğŸ’¾ Metadata saved: {METADATA_PATH.name}")

    # Step 13: Feature importance chart
    save_feature_importance_chart(result["feature_importance"])
    print(f"ğŸ’¾ Chart saved: {CHART_PATH.name}")


def save_feature_importance_chart(importance: dict):
    """à¸ªà¸£à¹‰à¸²à¸‡ horizontal bar chart à¹à¸ªà¸”à¸‡ feature importance"""
    names = list(importance.keys())
    values = list(importance.values())

    # Reverse à¹€à¸à¸·à¹ˆà¸­à¹ƒà¸«à¹‰ feature à¸ªà¸³à¸„à¸±à¸à¸ªà¸¸à¸”à¸­à¸¢à¸¹à¹ˆà¸šà¸™
    names.reverse()
    values.reverse()

    fig, ax = plt.subplots(figsize=(8, 5))
    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(names)))
    ax.barh(names, values, color=colors, height=0.6)

    ax.set_xlabel("Importance Score")
    ax.set_title("Feature Importance â€” XGBoost Model")
    ax.spines["top"].set_visible(False)
    ax.spines["right"].set_visible(False)

    for i, v in enumerate(values):
        ax.text(v + 0.005, i, f"{v:.4f}", va="center", fontsize=9)

    plt.tight_layout()
    plt.savefig(CHART_PATH, dpi=150, bbox_inches="tight")
    plt.close(fig)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def main():
    print_header()

    try:
        # Load from MongoDB
        applications = await load_data_from_mongodb()
        if not applications:
            print("âŒ No applications with HR decisions found")
            print("   â†’ à¹ƒà¸«à¹‰ HR Accept/Reject à¸œà¸¹à¹‰à¸ªà¸¡à¸±à¸„à¸£à¸à¹ˆà¸­à¸™ à¹à¸¥à¹‰à¸§à¸„à¹ˆà¸­à¸¢ train")
            return

        # Extract features
        X, y = extract_features(applications)
        if X.empty:
            print("âŒ Could not extract features from applications")
            return

        # Validate
        if not validate_data(X, y):
            return

        # Train & evaluate
        result = train_and_evaluate(X, y)

        # Save
        save_model(result)

        print("\n" + "â”" * 50)
        print("âœ… Done! XGBoost model is ready.")
        print("â”" * 50)

    except Exception as e:
        print(f"\nâŒ Training failed: {e}")
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
